{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# SUDA2 ALGORITHM IMPLEMENTATION\n",
        "\n",
        "# This script computes SUDA (Special Uniques Detection Algorithm) scores on stratified samples\n",
        "# to measure privacy disclosure risk in census microdata\n",
        "\n",
        "import pandas as pd\n",
        "import itertools\n",
        "from multiprocessing import Pool, cpu_count\n",
        "from collections import defaultdict\n",
        "import math\n",
        "\n",
        "# Global variable for maximum MSU (Minimal Sample Unique) size\n",
        "MAX_MSU_SIZE = None\n",
        "\n",
        "def normalization_constant(k):\n",
        "    \"\"\"Calculate normalization constant for k variables\"\"\"\n",
        "    return sum(math.comb(k, r) * (2 ** (k - r)) for r in range(1, k + 1))\n",
        "\n",
        "def detect_msus(df, key_vars, max_msu_size):\n",
        "    \"\"\"\n",
        "    Core SUDA algorithm: detects Minimal Sample Uniques (MSUs)\n",
        "\n",
        "    MSUs are the smallest combinations of variables that uniquely identify individuals.\n",
        "    For each record, we find all minimal combinations of quasi-identifiers that\n",
        "    make that record unique in the sample.\n",
        "    \"\"\"\n",
        "    k = len(key_vars)  # Total number of quasi-identifier variables\n",
        "    msu_results = []   # Store all detected MSUs\n",
        "    variable_contributions = defaultdict(int)  # Track how much each variable contributes to risk\n",
        "\n",
        "    # Initialize record-level scoring structure\n",
        "    record_scores = defaultdict(lambda: {\n",
        "        \"suda_score\": 0,           # Total SUDA score for this record\n",
        "        \"msu_counts\": defaultdict(int),  # Count of MSUs by size\n",
        "        \"msus\": [],                # List of all MSUs for this record\n",
        "        \"variables\": defaultdict(int)    # Variable-specific contributions\n",
        "    })\n",
        "\n",
        "    # Search through all possible combinations of variables, from size 1 to max_msu_size\n",
        "    for r in range(1, min(k, max_msu_size) + 1):\n",
        "        # Generate all combinations of r variables from the key variables\n",
        "        for cols in itertools.combinations(key_vars, r):\n",
        "            # Group records by current variable combination and count occurrences\n",
        "            grouped = df.groupby(list(cols)).size()\n",
        "            # Find combinations that appear exactly once (unique combinations)\n",
        "            unique_combos = grouped[grouped == 1].index\n",
        "\n",
        "            # Handle the case where we have only one grouping variable\n",
        "            if not isinstance(unique_combos, pd.MultiIndex):\n",
        "                unique_combos = [(val,) for val in unique_combos]\n",
        "            else:\n",
        "                unique_combos = list(unique_combos)\n",
        "\n",
        "            # Process each unique combination\n",
        "            for uc in unique_combos:\n",
        "                # Find the specific record that has this unique combination\n",
        "                mask = (df[list(cols)] == pd.Series(uc, index=cols)).all(axis=1)\n",
        "                matching_rows = df[mask]\n",
        "                # Skip if somehow more than one record matches (shouldn't happen)\n",
        "                if matching_rows.shape[0] != 1:\n",
        "                    continue\n",
        "\n",
        "                record_idx = matching_rows.index[0]\n",
        "                uc_dict = dict(zip(cols, uc))\n",
        "\n",
        "                # Check if this combination is minimal (no subset also makes record unique)\n",
        "                is_minimal = True\n",
        "                if len(cols) > 1:  # Only check for minimality if combination has >1 variable\n",
        "                    # Test all smaller subsets of current combination\n",
        "                    for sub_r in range(1, len(cols)):\n",
        "                        for sub_cols in itertools.combinations(cols, sub_r):\n",
        "                            sub_uc = tuple(uc_dict[c] for c in sub_cols)\n",
        "                            sub_mask = (df[list(sub_cols)] == pd.Series(sub_uc, index=sub_cols)).all(axis=1)\n",
        "                            # If any subset is also unique, current combination is not minimal\n",
        "                            if df[sub_mask].shape[0] == 1:\n",
        "                                is_minimal = False\n",
        "                                break\n",
        "                        if not is_minimal:\n",
        "                            break\n",
        "\n",
        "                # If combination is minimal, add to MSU results and calculate scores\n",
        "                if is_minimal:\n",
        "                    # Weight decreases as combination size increases (smaller = riskier)\n",
        "                    weight = 2 ** (k - r)\n",
        "\n",
        "                    # Store MSU information\n",
        "                    msu_results.append({\n",
        "                        'record_index': int(record_idx),\n",
        "                        'msu_combination': list(cols)\n",
        "                    })\n",
        "\n",
        "                    # Update record-level scores\n",
        "                    record_scores[int(record_idx)][\"suda_score\"] += weight\n",
        "                    record_scores[int(record_idx)][\"msu_counts\"][r] += 1\n",
        "                    record_scores[int(record_idx)][\"msus\"].append(list(cols))\n",
        "\n",
        "                    # Track variable-level contributions\n",
        "                    for v in cols:\n",
        "                        variable_contributions[v] += weight\n",
        "                        record_scores[int(record_idx)][\"variables\"][v] += weight\n",
        "\n",
        "    # Convert nested defaultdicts to regular dicts for JSON serialization\n",
        "    record_scores_serializable = {\n",
        "        int(rec_id): {\n",
        "            \"suda_score\": data[\"suda_score\"],\n",
        "            \"msu_counts\": dict(data[\"msu_counts\"]),\n",
        "            \"msus\": data[\"msus\"],\n",
        "            \"variables\": dict(data[\"variables\"]),\n",
        "        } for rec_id, data in record_scores.items()\n",
        "    }\n",
        "    var_contrib_serializable = dict(variable_contributions)\n",
        "    return msu_results, record_scores_serializable, var_contrib_serializable\n",
        "\n",
        "def parallel_suda(df, key_vars, max_msu_size):\n",
        "    \"\"\"\n",
        "    Wrapper function for parallel processing of SUDA algorithm\n",
        "    Currently uses single process but structure allows for easy parallelization\n",
        "    \"\"\"\n",
        "    # Use all available CPU cores minus one\n",
        "    num_cores = max(1, cpu_count() - 1)\n",
        "    pool = Pool(num_cores)\n",
        "\n",
        "    # Create work chunks (currently just one chunk containing full dataset)\n",
        "    chunks = [(df, key_vars, max_msu_size)]\n",
        "    results = pool.starmap(detect_msus, chunks)\n",
        "    pool.close()\n",
        "    pool.join()\n",
        "\n",
        "    # Combine results from all worker processes\n",
        "    all_msus = []\n",
        "    combined_scores = defaultdict(lambda: {\n",
        "        \"suda_score\": 0,\n",
        "        \"msu_counts\": defaultdict(int),\n",
        "        \"msus\": [],\n",
        "        \"variables\": defaultdict(int)\n",
        "    })\n",
        "    var_contrib = defaultdict(int)\n",
        "\n",
        "    # Merge results from all chunks\n",
        "    for msus, record_scores, var_scores in results:\n",
        "        all_msus.extend(msus)\n",
        "        for rec_id, val in record_scores.items():\n",
        "            combined_scores[rec_id][\"suda_score\"] += val[\"suda_score\"]\n",
        "            for k, v in val[\"msu_counts\"].items():\n",
        "                combined_scores[rec_id][\"msu_counts\"][k] += v\n",
        "            combined_scores[rec_id][\"msus\"].extend(val[\"msus\"])\n",
        "            for var, v in val[\"variables\"].items():\n",
        "                combined_scores[rec_id][\"variables\"][var] += v\n",
        "        for var, v in var_scores.items():\n",
        "            var_contrib[var] += v\n",
        "\n",
        "    return all_msus, combined_scores, var_contrib\n",
        "\n",
        "def generate_output(df, key_vars, msus, record_scores, var_contrib, max_msu_size,\n",
        "                    output_prefix=\"suda_output\"):\n",
        "    \"\"\"\n",
        "    Generate SUDA output files in standard format\n",
        "\n",
        "    Creates:\n",
        "    - Summary file with dataset statistics\n",
        "    - Records CSV with individual SUDA scores and metrics\n",
        "    - Attribute contribution file showing variable importance\n",
        "    \"\"\"\n",
        "    k = len(key_vars)\n",
        "    n_records = len(df)\n",
        "    normalization_const = normalization_constant(k)\n",
        "    sample_unique_count = len(record_scores)\n",
        "    num_msus = len(msus)\n",
        "    total_score = sum(r[\"suda_score\"] for r in record_scores.values())\n",
        "\n",
        "    # Create summary statistics file\n",
        "    with open(f\"{output_prefix}_summary.txt\", \"w\") as f:\n",
        "        f.write(\"+++++++++++++++++++++++++++++++++++++++++++++++++++\\n\")\n",
        "        f.write(\"Running SUDA with debug level=0\\n\")\n",
        "        f.write(\"file:___suda-processedData_.txt\\n\")\n",
        "        f.write(\"IDs:1\\n\")\n",
        "        f.write(\"detailed output:1\\n\")\n",
        "        f.write(\"samp. fract:0.010000\\n\")\n",
        "        f.write(f\"set={n_records} att={k}\\n\")\n",
        "        f.write(f\"max_range={df.shape[1]} max_records={n_records}\\n\")\n",
        "        f.write(\":::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::\\n\")\n",
        "        f.write(\"<<<<< Loading data - may take a few minutes >>>>>\\n\")\n",
        "        f.write(\"Attribute details:\\n\")\n",
        "        # Document each variable's range for reproducibility\n",
        "        for idx, col in enumerate(key_vars, start=1):\n",
        "            f.write(f\"{idx} col#{idx+1} att '{col}' min value {df[col].min()} max value {df[col].max()}\\n\")\n",
        "        f.write(f\"Search for minimal sample uniques up to size {max_msu_size} from {k} attributes\\n\")\n",
        "        f.write(f\"***The number of records read in is {n_records}***\\n\")\n",
        "        f.write(\"Applying SUDA...\\n\")\n",
        "        f.write(\":::::DIS_SUDA DETAILS::::::::::::::::::::::::::::::::::::::::::::\\n\")\n",
        "        f.write(f\"Sample Unique records {sample_unique_count}\\n\")\n",
        "        f.write(f\"Records in pairs {num_msus}\\n\")\n",
        "        f.write(f\"P(cm|um) = {num_msus / n_records:.6f}\\n\")\n",
        "        f.write(\"+++++++++++++++++++++++++++++++++++++++++++++++++++\\n\")\n",
        "\n",
        "    # Generate detailed record-level output\n",
        "    record_data = []\n",
        "    for idx, data in record_scores.items():\n",
        "        suda_score = data[\"suda_score\"]\n",
        "\n",
        "        # Calculate proportion of lattice (normalized SUDA score)\n",
        "        proportion_lattice = suda_score / math.factorial(k) if k > 0 else 0\n",
        "\n",
        "        # Count MSUs by their size (1-variable, 2-variable, etc.)\n",
        "        msu_sizes = {\n",
        "            f\"MSU_size_{i}\": data[\"msu_counts\"].get(i, 0)\n",
        "            for i in range(1, max_msu_size + 1)\n",
        "        }\n",
        "\n",
        "        # Calculate percentage contribution of each variable to this record's risk\n",
        "        var_contribs = {\n",
        "            f\"{v}_contribution_%\": round(w / suda_score * 100, 2) if suda_score > 0 else 0\n",
        "            for v, w in data[\"variables\"].items()\n",
        "        }\n",
        "\n",
        "        # Compile all metrics for this record\n",
        "        row = {\n",
        "            \"ID\": idx,\n",
        "            \"suda_score\": suda_score,\n",
        "            \"proportion_of_lattice\": round(proportion_lattice, 6)\n",
        "        }\n",
        "        row.update(msu_sizes)\n",
        "        row.update({f\"{var}_contribution_%\": var_contribs.get(f\"{var}_contribution_%\", 0) for var in key_vars})\n",
        "\n",
        "        record_data.append(row)\n",
        "\n",
        "    # Save record-level results to CSV\n",
        "    pd.DataFrame(record_data).to_csv(f\"{output_prefix}_records.csv\", index=False)\n",
        "\n",
        "    # Generate variable-level contribution summary\n",
        "    with open(f\"{output_prefix}_attribute_contrib.txt\", \"w\") as f:\n",
        "        for var in key_vars:\n",
        "            # Calculate what percentage of total risk this variable contributes\n",
        "            percent = var_contrib[var] / total_score * 100 if total_score > 0 else 0\n",
        "            f.write(f\"col#{key_vars.index(var)+2} att '{var}' percentage contribution {percent:.4f}\\n\")\n",
        "\n"
      ],
      "metadata": {
        "id": "RTRzCbRt-lSo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# EXECUTING ALGORITHM ON SAMPLES\n",
        "\n",
        "# Load stratified sample 2 data\n",
        "data = pd.read_csv(\"s1_ns2011.csv\")\n",
        "\n",
        "# Run SUDA algorithm on all samples\n",
        "if __name__ == \"__main__\":\n",
        "    # Load the sample dataset\n",
        "    df = data\n",
        "\n",
        "    # Define quasi-identifier variables (must match dataset column names)\n",
        "    # These are demographic variables that could potentially re-identify individuals\n",
        "    key_vars = [\"persons\", \"hhwt\", \"gq\", \"regionw\", \"ownershipd\"]\n",
        "\n",
        "    # Set maximum MSU size to consider all possible variable combinations\n",
        "    MAX_MSU_SIZE = len(key_vars)\n",
        "\n",
        "    # Execute SUDA analysis using parallel processing\n",
        "    msus, record_scores, var_contrib = parallel_suda(df, key_vars, MAX_MSU_SIZE)\n",
        "\n",
        "    # Generate comprehensive output files with results\n",
        "    generate_output(df, key_vars, msus, record_scores, var_contrib, MAX_MSU_SIZE, output_prefix=\"s1_ns2011\")"
      ],
      "metadata": {
        "id": "oWCl6LlC0O60"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TOTAL SUDA SCORE AND ERROR %\n",
        "\n",
        "# Load SUDA results from the generated CSV file\n",
        "df = pd.read_csv(\"s1_ns2011_records.csv\")\n",
        "\n",
        "# Calculate total SUDA score by summing individual record scores\n",
        "suda_total = df[\"suda_score\"].sum()\n",
        "print(suda_total)\n",
        "\n",
        "# Compare sample SUDA score against true population risk\n",
        "# Population risk of 15061 was calculated from the full census dataset\n",
        "population_risk = 15061\n",
        "\n",
        "# Calculate percentage error between sample estimate and true population value\n",
        "# Positive error = sample overestimates risk, Negative error = sample underestimates risk\n",
        "percent_error = ((suda_total - population_risk) / population_risk) * 100\n",
        "print(f\"% Error: {percent_error:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lmKyOATs0inu",
        "outputId": "b18514e8-bd7c-4211-9652-17db149795db"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "29872\n",
            "% Error: 98.34%\n"
          ]
        }
      ]
    }
  ]
}